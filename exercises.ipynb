{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c36ca11",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "source": [
    "# Workshop: Generating Privacy-Preserving Synthetic Data with PrivBayes\n",
    "\n",
    "Welcome to this hands-on workshop! In this session, we will explore privacy-preserving data generation using [PrivBayes](https://github.com/XYZ/PrivBayes), a powerful tool for creating static tabular synthetic datasets while preserving privacy. \n",
    "\n",
    "Imagine you are a data scientist working for a healthcare organization. You have access to a dataset containing sensitive patient information, but due to privacy regulations, you cannot share this data directly with external researchers or use it in public-facing applications. However, you still want to enable meaningful analysis and collaboration. How can you achieve this without compromising privacy?\n",
    "\n",
    "In this workshop, you will:\n",
    "1. **Generate Synthetic Data**: Use PrivBayes to create a synthetic version of a sensitive dataset. This synthetic data will retain the statistical properties of the original data while ensuring that individual records cannot be re-identified.\n",
    "2. **Measure Privacy**: Evaluate the privacy guarantees of the synthetic data you produce. You will learn how to quantify privacy leakage and understand the trade-offs between data utility and privacy.\n",
    "\n",
    "By the end of this session, you will have hands-on experience with privacy-preserving data generation techniques and a deeper understanding of how to balance privacy and utility in real-world scenarios. Let's get started by installing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2966401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad4d9c",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 1: Sensitive Dataset Insight\n",
    "\n",
    "In this session we will be working on a subset of the ALS static-vars dataset. \n",
    "Lets load the data and get some insights:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb3a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Load the dataset\n",
    "real_data = pd.read_csv('Data/filtered_datasetC_train-static-vars.csv')\n",
    "\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "display(real_data.head().style.hide(axis=\"index\"))\n",
    "print(\"\\n Number of individuals in the dataset:\", len(real_data))\n",
    "print(\"\\n Description of continuous attributes:\")\n",
    "display(real_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826e7df",
   "metadata": {},
   "source": [
    "Lets use t-distributed stochastic neighbor embedding (t-SNE) to visualize the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57acb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Function to create a scatter plot of the real data using t-SNE\n",
    "def scatter_plot_real_tsne(dataset):\n",
    "    tsne = TSNE(n_components=2)\n",
    "    coords = tsne.fit_transform(dataset)\n",
    "    coords_df = pd.DataFrame(coords)\n",
    "    # Scatter Plot\n",
    "    plt.figure()\n",
    "    # Plot DataFrame 1\n",
    "    plt.scatter(coords_df[0], coords_df[1], color='royalblue',marker='.',label='Real', alpha=0.5)\n",
    "    \n",
    "    plt.title('Scatter plot of real data (t-SNE)')\n",
    "    plt.xlabel('X-coord')\n",
    "    plt.ylabel('Y-coord')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    return plt\n",
    "\n",
    "encoded_real_data = pd.get_dummies(real_data) # One-hot encoding of categorical variables\n",
    "\n",
    "# Scatter plot of real data\n",
    "scatter_plot_real_tsne(encoded_real_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a9d8e",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 2: Synthesizing the Dataset with PrivBayes\n",
    "\n",
    "In this exercise, we will use PrivBayes to generate a synthetic version of our dataset.\n",
    "\n",
    "PrivBayes is an $\\varepsilon$-differentially private synthetic data generator mechanism that synthesizes data using Bayesian networks through a three-step process:\n",
    "1. **Construct Bayesian network**: it selects a Bayesian network by sampling from a distribution over possible network structures, where the probability of each structure is proportional to a Laplace-weighted scoring function (e.g., based on mutual information) evaluated on the confidential data;\n",
    "2. **Make noisy conditional distributions (CPT)**: it injects Laplace noise into the CPTs to ensure differential privacy; and\n",
    "3. **Synthetic data sampling**: it generates synthetic records by sequentially sampling attribute values according to the noisy conditional distributions, following the topological order of the Bayesian network.\n",
    "\n",
    "When using PrivBayes, you will be shown information about the dataset as well as the learned Bayesian Network (BN). \n",
    "1. **Does the network logicaly make sense?**\n",
    "2. **Try changing the $\\varepsilon$ (epsilon) parameter.** Does increasing the $\\varepsilon$-value make the dependencies in the BN more realistic? What happens when you decrease the $\\varepsilon$-value?\n",
    "\n",
    "Let's begin generating some synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff7cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthesis.synthesizers.privbayes import PrivBayes\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to synthesize data\n",
    "def synthesize(dataset, epsilon):\n",
    "    # instantiate and fit synthesizer\n",
    "    pb = PrivBayes(epsilon=epsilon, verbose=False, theta_usefulness=0.15, epsilon_split=0.5)\n",
    "    pb.fit(dataset)\n",
    "    \n",
    "    # Synthesize data\n",
    "    gen_data  = pb.sample()\n",
    "\n",
    "    edges = pb.model_.edges\n",
    "    # Save to csv file\n",
    "    syn_data = pd.DataFrame(gen_data.values, columns=gen_data.columns, index=range(dataset.shape[0]))\n",
    "     \n",
    "    return syn_data, edges\n",
    "\n",
    "# Create directed graph\n",
    "def bn_plot(edges):\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    pos = nx.shell_layout(G) \n",
    "    nx.draw(G, pos, with_labels=True, node_size=500, node_color='lightblue', arrows=True, font_size=10)\n",
    "    nx.draw_networkx_edges(G, pos, arrowsize=20)\n",
    "    plt.title(\"Bayesian Network Structure\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Synthesize data\n",
    "synthetic_data, edges = synthesize(real_data, epsilon=10)\n",
    "\n",
    "# Save the synthetic dataset\n",
    "synthetic_data.to_csv('Data/synthetic_dataset.csv', index=False)\n",
    "\n",
    "print(\"Synthetic dataset saved to 'Data/synthetic_dataset.csv'\")\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "display(synthetic_data.head().style.hide(axis=\"index\"))\n",
    "print(\"\\n Number of individuals in the dataset:\", len(synthetic_data))\n",
    "cont_attributes = [\"onsetDate\", \"diagnosisDate\", \"height\", \"weight\"]\n",
    "print(\"\\n Description of continuous attributes:\")\n",
    "synthetic_data[cont_attributes] = synthetic_data[cont_attributes].astype(float)\n",
    "display(synthetic_data.describe())\n",
    "\n",
    "\n",
    "bn_plot(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d369d512",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 3: Analysing the utility of your synthetic dataset\n",
    "\n",
    "Let us investigate the utility of the synthetic data. \n",
    "\n",
    "To assist you in analysing the utility of your synthetic data, we provide the tools:\n",
    "1. **t-SNE Scatter Plot**: A visualization of the high-dimensional data in a 2D space to observe the clustering and distribution of real and synthetic data points.\n",
    "2. **KL-Divergence (per attribute)**: Measures how different the **categorical** distributions are between real and synthetic data for a single feature. (0 = identical distributions; higher = more divergence)\n",
    "3. **Wasserstein Distance**: Measures how much the **continuous** value distributions differ (accounts for magnitude and shape). (0 = very similar; higher = more different)\n",
    "4. **Correlation Difference**: The average difference in pairwise correlations between variables in the real vs synthetic datasets. (0 = same relationships; higher = structure distortion)\n",
    "\n",
    "Utilitsing these tools;\n",
    "\n",
    "**Try changing the $\\varepsilon$-value** of the synthesize function and investigate how the change influences the utility.\n",
    "\n",
    "Try to use the tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fed3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy, wasserstein_distance\n",
    "\n",
    "# --- Detect column types\n",
    "def detect_column_types(df, threshold=10):\n",
    "    categorical = []\n",
    "    continuous = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object or df[col].nunique() <= threshold:\n",
    "            categorical.append(col)\n",
    "        else:\n",
    "            continuous.append(col)\n",
    "    return categorical, continuous\n",
    "\n",
    "# --- KL divergence\n",
    "def compute_kl_divergence(col, real_df, synth_df):\n",
    "    p = real_df[col].value_counts(normalize=True).sort_index()\n",
    "    q = synth_df[col].value_counts(normalize=True).sort_index()\n",
    "    p, q = p.align(q, fill_value=1e-8)\n",
    "    return entropy(p, q)\n",
    "\n",
    "# --- Correlation difference\n",
    "def pairwise_correlation_diff(df1, df2):\n",
    "    corr1 = df1.corr()\n",
    "    corr2 = df2.corr()\n",
    "    return (corr1 - corr2).abs().mean().mean()\n",
    "\n",
    "# --- Wasserstein distance for continuous variables\n",
    "def compute_wasserstein_dist(col, real_df, synth_df):\n",
    "    return wasserstein_distance(real_df[col], synth_df[col])\n",
    "\n",
    "# --- Wrap all metrics\n",
    "def compute_unsupervised_utility(real_df, synth_df):\n",
    "    categorical, continuous = detect_column_types(real_df)\n",
    "\n",
    "    kl_divs = {col: compute_kl_divergence(col, real_df, synth_df) for col in categorical}\n",
    "    corr_diff = pairwise_correlation_diff(real_df[continuous], synth_df[continuous]) if continuous else None\n",
    "    wass_dists = {col: compute_wasserstein_dist(col, real_df, synth_df) for col in continuous}\n",
    "\n",
    "    return kl_divs, corr_diff, wass_dists\n",
    "\n",
    "# --- Scatter plot of real and synthetic data\n",
    "def scatter_plot_tsne(real_data, syn_data):\n",
    "    tsne = TSNE(n_components=2)\n",
    "    real_coords = tsne.fit_transform(real_data)\n",
    "    syn_coords = tsne.fit_transform(syn_data)\n",
    "    # Create DataFrames for the coordinates\n",
    "    real = pd.DataFrame(real_coords)\n",
    "    syn = pd.DataFrame(syn_coords)\n",
    "    \n",
    "    # Scatter Plot\n",
    "    plt.figure()\n",
    "\n",
    "    # Plot DataFrame 1\n",
    "    plt.scatter(real[0], real[1], marker='.', color='royalblue', label='Real', alpha=0.5)\n",
    "\n",
    "    # Plot DataFrame 2\n",
    "    plt.scatter(syn[0], syn[1], marker=7,color='red', label='Synthetic', alpha=0.5)\n",
    "    \n",
    "\n",
    "    plt.title('Scatter plot of real and synthetic data (t-SNE)')\n",
    "    plt.xlabel('X-coord')\n",
    "    plt.ylabel('Y-coord')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    return plt\n",
    "\n",
    "kl_divs, corr_diff, wass_dists = compute_unsupervised_utility(real_data, synthetic_data)\n",
    "\n",
    "# Print clean table\n",
    "results = pd.DataFrame(\n",
    "    [{\"Metric\": f\"KL Divergence ({k})\", \"Value\": v} for k, v in kl_divs.items()] +\n",
    "    [{\"Metric\": f\"Wasserstein Distance ({k})\", \"Value\": v} for k, v in wass_dists.items()] +\n",
    "    [{\"Metric\": \"Correlation Difference\", \"Value\": corr_diff}]\n",
    ")\n",
    "\n",
    "display(results.style.hide(axis=\"index\"))\n",
    "\n",
    "encoded_synthetic_data = pd.get_dummies(synthetic_data) # One-hot encoding of categorical variables\n",
    "\n",
    "# Scatter plot of real and synthetic data\n",
    "scatter_plot_tsne(encoded_real_data, encoded_synthetic_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f9c067",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise 4: Analysing the privacy of your synthetic dataset\n",
    "\n",
    "Let us now measure the privacy of the synthetic data using the metrics covered in the presentation. \n",
    "\n",
    "To assist you in analysing the privacy of your synthetic data, we provide the metrics:\n",
    "1. **Attribute Inference Risk (AIR)**: Assesses how easily an attacker, using public real data and synthetic data, can infer sensitive attributes (for illustrative purposes, we've set the sensitive attribute as a binary attribute (smoking), but you can adjust this if you want to in the *sensitive_attributes.txt* file). It quantifies this difficulty with the a weighted F1-score.\n",
    "2. **Distance to Closest Record (DCR)**: Measures how close each synthetic record is to its nearest real record in a 2-d space created using PCA (Smaller distances may indicate privacy risk due to overfitting or memorization).\n",
    "3. **Nearest Neighbour Distance Ratio (NNDR)**: Compares the distance from a synthetic record to the nearest real vs. nearest synthetic record in a PCA derived 2-d space (Values near 0 suggest synthetic data is well-separated from real data (better privacy).\n",
    "4. **Nearest Synthetic Neighbour Distance (NSND)**: Average min-max reduced distance from each real record to its nearest synthetic neighbour.\n",
    "\n",
    "*These metrics have their score adjusted such that a score close to 1 means high risk and close to 0, no risk.*\n",
    "\n",
    "Utilitsing these metrics try to answer the following questions;\n",
    "\n",
    "1. **What do these results tell us?** Is the data private? How can we determine whether the data is private?\n",
    "2. **Try changing the $\\varepsilon$-value** of the synthesize function and investigate how the change influences the privacy.\n",
    "\n",
    "Try to use the tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5fa776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PrivacyMetrics import AttributeInference1 as AIR\n",
    "from PrivacyMetrics import DCR\n",
    "from PrivacyMetrics import NNDR\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# --- Attribute Inference\n",
    "def get_metric_results(real_data, synthetic_data):\n",
    "    # Calculate the attribute inference metric\n",
    "    air = AIR.calculate_metric(args = None, _real_data=real_data, _synthetic=synthetic_data)\n",
    "    dcr = DCR.calculate_metric(args = None, _real_data=real_data, _synthetic=synthetic_data)\n",
    "    nndr = NNDR.calculate_metric(args = None, _real_data=real_data, _synthetic=synthetic_data)\n",
    "    # Create a dictionary to store the results\n",
    "    results = {\n",
    "        \"AIR\": air,\n",
    "        \"DCR\": dcr,\n",
    "        \"NNDR\": nndr\n",
    "    }\n",
    "    clear_output(wait=False)\n",
    "    return round(pd.DataFrame(results, index=[0]), 2)\n",
    "\n",
    "# --- Calculate the metrics\n",
    "results = get_metric_results(real_data, synthetic_data)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nPrivacy Metric Results:\")\n",
    "display(results.style.hide(axis=\"index\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e31793",
   "metadata": {},
   "source": [
    "---\n",
    "# Final notes: Privacy-Preserving Synthetic Data Workshop\n",
    "\n",
    "### What We Did\n",
    "\n",
    "- Generated synthetic datasets from real data using a **differentially private mechanism** (PrivBayes).\n",
    "- Systematically varied the **privacy budget ($\\varepsilon$)** to control the level of noise and privacy.\n",
    "- Measured how **increasing/decreasing noise**:\n",
    "\n",
    "  - Affects **data utility** (how useful or similar the synthetic data is).\n",
    "  - Impacts **privacy guarantees** (how well individual records are protected).\n",
    "\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **Utility–Privacy Tradeoff**:\n",
    "  - Lower ε (more noise) → better privacy, but reduced statistical utility.\n",
    "  - Higher ε (less noise) → more realistic synthetic data, but weaker privacy.\n",
    "\n",
    "- **Utility Evaluation**:\n",
    "  - Visualisation: t-SNE plot.\n",
    "  - Utility metrics: KL divergence, correlation difference, Wasserstein distance.\n",
    "\n",
    "- **Privacy Evaluation**:\n",
    "  - Empirical metrics: AIR, DCR, NNDR, NSND.\n",
    "    - Increasing noise reduces privacy leakage — but also utility.\n",
    "\n",
    "## Takeaways\n",
    "\n",
    "- **Synthetic data can balance utility and privacy**, but the optimal $\\varepsilon$ depends on the use case.\n",
    "- Evaluation should be **multi-dimensional**:\n",
    "  - Statistical similarity\n",
    "  - Predictive performance\n",
    "  - Privacy leakage\n",
    "  \n",
    "- There is no single \"best\" $\\varepsilon$ — it must align with:\n",
    "  - Data sensitivity\n",
    "  - Risk tolerance\n",
    "  - Regulatory constraints (which you from **exercise 4** may have experienced to be quite difficult)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
